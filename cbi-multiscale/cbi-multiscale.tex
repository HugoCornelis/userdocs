\documentclass[12pt]{article}
\usepackage{verbatim}
\usepackage[dvips]{epsfig}
\usepackage{color}
\usepackage{url}
\usepackage[colorlinks=true]{hyperref}

\begin{document}

\section*{GENESIS: Documentation}

{\bf Related Documentation:}
% start: userdocs-tag-replace-items related-do-nothing
% end: userdocs-tag-replace-items related-do-nothing

\section*{Multiscale Modeling with a CBI Based Simulator}

\subsection*{structure}

- problem statement: multi-scale modeling requires a single model-container

- explain CBI architecture

- apply: G-3 and scripting

- explain communication infrastructure for multi-scale modeling


\subsection*{Introduction}

\subsection*{The User Workflow}

The term {\it user workflow} is employed to describe the sequence of
necessary steps typically employed by a person in developing a
computational model and employing simulation to generate data for
subsequent analysis. In this sense it is a depiction of a sequence of
operations, declared as the work of a person or a group of persons
\cite{Belhajjame:2001fv}.

A comprehensive user workflow can be employed to guide the separation
of the different aspects of a model by organizing user actions into
different categories during model development.  The workflow allows
distinctions to be made between an object under investigation, the
tools used to perform the investigation, and the operations performed
during the investigation. It also distinguishes between the results
obtained from a single investigation and the method used to define
multiple investigations in a series. The user workflow identifies five
steps in total (explained in more detail in Results).

\subsubsection*{The Ideal User Workflow for Simulations in Neurobiology}

As many more data flows can exist than are present in reality, each actual
data flow can be considered in the context of a sequence of user
actions or workflow. We define an ``ideal user workflow'' that provides a canonical form
of a user workflow specific for neural simulators. In this section, we introduce
the set of typical workflows that use CBI simulator architecture
applications by describing the ideal user workflow where a user wants to model a biological system. We then briefly mention a second set of workflows that comprise user extensions of the
functionality of an implementation of the CBI architecture. Both sets of workflows are presented in a technology and implementation free manner.

%In this paper we discuss the first set of workflows by introducing an
%and we superficially touch upon the second
%set of workflows.  All workflows described in the following sections
%are technology and implementation free.
%Examples are given for
%purpose of illustrating the meaning of the text, and neither for
%illustrating the current (state of the existing) implementation, nor
%for a possible implementation in a specific technology.

A five step outline of an ideal user workflow for the development,
implementation, and simulation of a computational model has been
identified from the workflow of users of the GENESIS neural simulation
platform \cite{cornelis02:_tutor}.  Importantly, the workflow
explicitly distinguishes between the static structure of a model of
the biology (Step 1), the dynamic state of its simulation (Step 3),
and the analysis of this dynamic state (Step 4). We also note that
this workflow does not specify any particular order for its
completion. However, for any given case, meaningful simulation output
will only occur with completion of Steps 1--4.

\subsubsection*{Step 1: Construct Model}

The simulator shell and the graphical user interface (GUI) each
provide an interface that interprets user input such that the
simulator `understands' different commands and performs the
appropriate actions. Simple models can be created directly within the
simulator shell by entering a sequence of commands. More complex
models are available to the shell from libraries or databases external
to the simulator. Shell tools can then be used to explore and check
the integrity of a model. Following any necessary or desired changes,
a new version of the model can be saved.

\subsubsection*{Step 2: Design Experiment}

Specific change management tools can be used to make small
modification to a model, e.g. to set model parameter values specific
to a given simulation.  Configuration tools support the definition of
the stimulus or activation parameters for a given simulation run or
experiment and the output variables to be stored for subsequent
analysis by independent software.

\subsubsection*{Step 3: Run Simulation}

Shell tools can be used to check the state of a given simulation or reset the simulation time step and solved variables to their initial values. After a simulation is run, output values are flushed to raw result storage for subsequent data analysis. The model state can be saved at any simulation time step. This allows it to be imported into a subsequent simulator session for further development and exploration.

\subsubsection*{Step 4: Process Output}

The validity and location of simulator output is checked prior to data
analysis. Output can be analyzed either within the simulator or piped
to external applications such as
Matlab. % for subsequent data analysis.

\subsubsection*{Step 5: Iterate}

A modeling project is established by the introduction of iterators
into the user workflow. Iterators close the loop between the output of
results and model construction, they include: Automated construction
of simulations and batch files, static parameter searching, and active
parameter searching using, for example, dynamic clamp technology.


\subsection*{Multi-Scale Modeling and the User-Workflow}
The user-workflow is based on experimental paradigm and childishly
simple, but, in contrast, the number of available numerical techniques
is large.  As a consequence any tool will become useful as far as its
implicit or explicit capacity of simplifying the mapping from this
large set of techniques to the simple user-workflow.  Currently such a
tool does not exist.  This is at the core of the problem of
multi-scale simulation.

The major benefit of G-3 is that it succeeds in simplifying a
substantial part of the picture: running either a simulation of a
single neuron or of a network of multicompartmental neurons can be
done from the command line (and uses many implicit default values).
Advanced G-3 users can drill down into the details of and make changes
to either the model, the experiment or the simulation configuration.
How to expand the G-3 scope for multi-scale simulation is explained
below.

It is simple to run simulations from a command line.  As an example:

ssp --cell cells/purkinje/edsjb1994.ndf --inject-current 1e-9 --time 1

ssp --network networks/spiker3.ndf --network-input 1e-9 --time 1

Both these command lines follow a template of model specification,
experiment specification, simulation configuration.  Note that line 2
is essentially a simple multi-scale simulation that instantiates
solvers and run-time data communication components as necessary.  By
design this feature of the modular architecture of NS / G-3 works the
same within and across all scales.

The question 'given all the technology available, why is multi-scale
modeling so difficult?' is quickly and superficially answered after
the observation that simulations are limited by their engines and
solvers.

A good use case is necessary to make this understandable, ie the three
steps in your email.

$<$based on such as use case it is easy to make a diagram for further
clarification during a presentation about these three steps, and it
can have three 'growing' versions during the presentation where each
version of the diagram plugs in new software components into the
system$>$

I think a presentation should have the following implicit elements:

1. the user-workflow: a scientist first cares about the scientific
question, the technology is secondary.

2. the model and its computational expression have a central role from
the science viewpoint (so the requirement for having exactly one
model-container).  At its essence a scientific model is 'scale-less'
(but some people will not like this type of expression), or worded
differently, the expression of the model can be independent of scale,
and so can be multi-scale, or not (but a scientist does not care
because he simply implements the simple user-workflow during his daily
work).

3. the single model-container 'contains' the model with both
structures of components and values.  It has all the connections
between the different components of the model, independent of whether
these components are solved independently / in isolation or,
alternatively, by a single engine.

The G-3 plugin mechanism interfaces the (single) model-container with
multiple run-time simulation objects.  Superficially there is a
discrete event system (action potential propagation abstraction and
communication), a compartmental solver (crank-nicolson or simpler),
and may be a kinetic pathway solver (RK or equivalent).  This is what
G-2 offers (but without a rigid user-workflow).  When you go one level
down, we encounter a rich set of well understood equations that can be
solved deterministically or stochastically (Monte-Carlo or otherwise).
 This landscape is more complex, although mathematically relatively
well understood.

Some of the equation types in subcellular simulations:
- K-Epsilon
- Navier-Stokes
- Heat-equations
- Reynold-equations
- Helmholtz-equations
- Poisson-Boltzmann equations
(note that not all these names apply to neuroscience I can update the
list for neuroscience):

Looking one level up, we see an equally rich set of gamble-and-win
solution methods:

Some of the model-types in supranetwork (large scale) simulations:
- IaF
- Adaptive IaF
- Izhekevic IaF
- Conductance based synapses
- Current based synapses
- Convolution and Waveform dependent synaptic weights

Two conclusions can be drawn: firstly from the biology point of view
the single neuron level seems to be the level that is best understood,
the anchor point for further expansion.  Firstly, in comparison to
most of the other software tools, this puts G-3 in an excellent
strategical position.  Both the Neuron and Moose simulators seem to be
in slightly different but mostly equivalent positions.

Secondly it is clear that the old generation of simulators (not
including G-3) does not offer the flexibility required to suit the
next generation of multi-scale models because of the complexity of the
available solution methods.  In other words current software
technology limits the scope of the research.  Many people agree with
the statement, but few truly understand and even fewer are trying to
change that situation, so I think it is important to make that point
clear.

$<$technical intermezzo$>$

Besides our efforts there are two other efforts that I am aware of
that try to solve this problem.  Firstly Neuron now includes
additional solvers for subcellular modeling.  I have strong personal
doubts that this will be truly successful for two reasons: the Neuron
environment does not have a mature plugin mechanism (read: it is
previous generation).  As a consequence the new solvers are tightly
integrated with the rest of the code and new solvers cannot be
contributed by external developers.  It also seems that the networking
capabilities in Neuron are still immature from a usability
perspective.  Overall I have the impression that Neuron is at the
limits of its internal (monolythic) architecture.

Secondly Moose also has a plugin architecture that allows developers
to plugin new solvers but it is immature.  The API is unstable
(changes over time) and is hardly documented.  The API is based on the
mathematics of the model (via the Moose messaging system) which is
conceptually similar to G-2 and I consider it a limiting factor.

In part because of these problems with Moose and Neuron for
multi-scale simulation, it is clear now that advanced middle-ware is /
will be necessary for the implementation of efficient communication
between solvers.  Currently people try to hack in the middle-ware as
necessary, but neither Neuron nor Moose nor any other simulator I
know, offers the handles necessary to manage this type of middle-ware
at run-time.  G-3 does and it works.  As a proof of concept, Mando
recently started to interface with these handles without understanding
their full scope (because he does not have to).  But except for the
model-container there are no G-3 components nor middle-ware available
for running multi-scale simulations.  So it currently stays a proof of
concept.

$<$end technical intermezzo$>$

For a presentation:

1. Contrast the complexity of a Purkinje cell (early in the
presentation) with that of the user-workflow-to-simulation mapping
(later in the presentation, eg. a slide that lists some of the
available solution methods, I do have such a slide somewhere in one of
my own presentations, let me know whether you would like to see / use
it).

2. The complexity we are facing is similar to multi-scale simulation
in other fields.  Nevertheless the pressure of the complexity of
biology by itself has a multiplicative factor to the already present
mathematical complexity.

Some additional interesting points:

1. The degree of how much a G-3 user drills down into technical
details of a simulation puts in him in one of the categories of either
student, teacher or researcher role, essentially on a mapping ranging
from the most naive to the most advanced role.  G-3 makes this
distinction rather intuitive.  The documentation system expands on
this idea.

2. Complementary to the scales starting from molecules upto system
networks, and I think as interesting, is the comparative study.

Let me know if I can be of further help.  Specifically if you want me
to send you some slides.

\subsection*{Methods}

Scientific workflow -- user workflow.

CBI-architecture.

CBI-behavioral view.

CBI-scripting, python scripting.


\subsection*{Results}

- Chemesis CNS report.

- integration heccer and chemesis into the gshell.


\subsection*{Discusion}

- Summary.

- Parts of the multiscale grant.


\subsection*{text sources}

- email jim single model -container

- multiscale grant

- dave's chapter

- chemesis cns report

- two papers


\subsection*{additional references}

- dave's chapter

- visual interface to neuron (see Jim's review invitation)

- numerical solvers, see Andrew Davison's email.


%\begin{figure}[h]
%  \centering
%   \includegraphics[scale=0.5]{figures/dummyfig.eps}
%\caption{{\bf A Dummy Figure:} Example of \LaTeX\,\,\,code to incorporate a figure into documentation.}
%  \label{fig:df-2}
%\end{figure}

\bibliographystyle{plain}
\bibliography{../tex/bib/g3-refs.bib}

\end{document}
