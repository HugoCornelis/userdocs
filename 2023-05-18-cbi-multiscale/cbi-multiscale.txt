Definitions:
http://www.cs.cmu.edu/~dst/NIPS/nips08-workshop/Mike_Arnold.pdf
Multiple sub-models at differing abstractions and time-scales can be combined into a single heterogeneous multi-scale model using both synchronous and asynchronous approaches.

http://math.la.asu.edu/~cans/tutorials.html
PSICS, http://www.psics.org
PSICS models the behavior of neurons taking account of the stochastic nature of ion channel gating and the detailed positions of the channels themselves. PSICS is intended to be complementary to existing tools. With its focus on kinetic scheme channel models, stochastic behavior, and detailed geometry, it lives in the space between stochastic diffusion models of small sections of neurons (MCell, STEPS) and deterministic whole cell models (Neuron, Genesis).

http://www.biomedcentral.com/1471-2202/10/S1/P54
Multiscale modeling and interoperability in MOOSE

http://www.imagwiki.nibib.nih.gov/mediawiki/index.php?title=Computational_Neuroscience_Working_Group
TITLE: What do we mean by multi-scale modeling: Report from CNS 2011 workshop in Stockholm.
SUMMARY - Drs. Bower and Rybak will report on a workshop on multiscale modeling in computational neuroscience held as part of the 20th annual International Computational Neuroscience meeting (CNS*11) held in Stockholm, Sweden in July, 2011. The workshop considered challenges facing multi-scale modeling in computational neuroscience and produced a schema intended to describe and organize multi-scale modeling development efforts. This schema will be used as a basis for considering the current state of multi-scale modeling in computational neuroscience, as well as areas for future development.

http://www.imagwiki.nibib.nih.gov/mediawiki/index.php?title=Multiscale_Systems_Biology_Working_Group

http://www.cnsorg.org/assets/docs/CNS_Workshops/workshop_multiscale.pdf

http://www.incf.org/core/programs/modeling/projects/standards
Multiscale modeling is a tool of critical importance for neuroscience. As computational modeling techniques become integrated with experimental neuroscience, more knowledge can be extracted from existing experimental data. Quantitative models assist in generating experimentally testable hypotheses and in selecting informative experiments. One major challenge in the field is that, because of a wide range of simulation tools being used in the community, it is unlikely that one laboratory can reproduce the results obtained by another group, even if the model is deposited in an openly accessible database. The absence of widely adopted standards for model description also hamper efforts to make existing programs more compatible, reduce opportunities for innovative software development and for benchmarking of existing simulators.

http://www.cccblog.org/2011/04/25/an-interagency-multiscale-modeling-initiative/
Multiscale models can be designed to integrate diverse data, create testable hypotheses leading to new investigational studies, identify and share gaps in knowledge, uncover biological mechanisms, or make predictions about clinical outcome or intervention effects.  These models can draw on a variety of data sources including relevant physical, environmental, clinical and population data. Ultimately multiscale models and the information derived from their use will enable biomedical, biological, behavioral, environmental and clinical researchers to understand complex biological and behavioral systems in a manner not possible through traditional research methods…
 
The ultimate goal of the models would be to make realistic scientific predictions to address problems and issues in the environment; in the human body (e.g., to prevent, diagnose and treat the diseases or aberrations in normal development, and/or to predict treatment outcomes); and among individuals, groups, and within populations.

http://www.neuroinformatics2010.org/incf-japan-node-special-symposium/incf-japan-node-session-abstracts
ERIK DE SCHUTTER

New model description standards to facilitate multi-scale modeling

Okinawa Institute of Science and Technology, Japan and University of Antwerp, Belgium

Multi-scale modeling is a tool of critical importance for neuroscience. As computational modeling techniques become integrated with experimental neuroscience, more knowledge can be extracted from existing experimental data. Quantitative models assist in generating experimentally testable hypotheses and in selecting informative experiments. One major challenge in the field is that, because of a wide range of simulation tools being used in the community, it is unlikely that one laboratory can reproduce the results obtained by another group, even if the model is deposited in an openly accessible database. The absence of widely adopted standards for model description also hamper efforts to make existing programs more compatible, reduce opportunities for innovative software development and for benchmarking of existing simulators.
The INCF has started a project to develop a new standard markup language for model description. Based on lessons learned with previous efforts in computational neuroscience and in other fields like systems biology, a concerted effort is made to develop a well-defined but flexible syntax for a self-documenting markup language that will be easy to extend and that can form the basis for specific implementations covering a wide range of modeling scales. The initial effort focuses on describing a growing area of computational neuroscience, spiking networks. This language, called NineML (Network Interchange format for NEuroscience) is based on a layered approach: an abstraction layer allows a full mathematical description of the models, including events and state transitions, while the user layer contains parameter values for specific models. The user layer includes concepts from the four major areas of neuroscience network modeling: neurons, synapses, populations and network connectivity rules. The abstraction layer includes notations for representing hybrid dynamical systems, combining differential equations with event based modeling to describe integrate-and-fire neurons, and abstract ma Cambridge: MA.thematical representations of connectivity rules. These abstractions are capable of describing a vast range of network models from the neuroscience literature.
The first official release of NineML is expected by the end of the year. In a next phase NineML will be improved by incorporating community feedback and by expanding the coverage of different spiking network models. Based on this experience new efforts will be launched in the future, using the same language syntax to cover other areas of computational neuroscience, including compartmental models, synaptic microphysiology, cellular mechanics, electrodynamics. Special attention will be given to interoperability issues relevant to multi-scale modeling, where many separate model descriptions may have to be combined and data interfaces need to be defined.

http://www.frontiersin.org/computational_physiology_and_medicine/10.3389/fphys.2011.00004/abstract


Churchland PS & Sejnowski TJ (1992) The Computational Brain. The MIT Press: Cambridge, MA.

http://books.google.com.au/books?id=wVll6u0tzXoC&printsec=frontcover&source=gbs_ge_summary_r&cad=0#v=onepage&q&f=false

p. 18
Levels in the Nervous System
Marr (1982) articulated a framework for a theory of levels that provided a background for thinking about levels in the context of computation by nervous structures. It drew upon the conception of levels in computer science and resulted in the characterisation of three levels: (1) the computational level of abstract problem analysis , decomposing the task into its main constituents, (2) the algorithmic level, specifying a formal procedure to perform the task so that for a given input, the correct output results, and (3) the level of physical implementation, constructing a working device given a particular technology.
S+C argue that Marr's division really corresponds to three different sorts of questions that can be raised about the phenomenon: (1) How does the problem decompose into parts, (2) what principles govern how the parts interact to solve the problem, and (3) what is the stuff who's causal interactions implement the principles?

Marr considered that a higher-level question was largely independent of the levels below it, thus computational problems of the highest level could be analysed independently of understanding the algorithm which performs the computation. Similarly, the algorithmic problem of the second level was thought to be solvable independently of understanding its physical implementation. Thus, his strategy was top-down rather than bottom-up.
However, two issues were confused in the doctrine of independence. One concerned whether, {\it as a matter of discovery}, the relevant algorithm and problem analysis can be figured out independently of facts about implementation. The other concerns whether, {\it as a matter of formal theory}, a given algorithm which is already known to perform a given task in a given machine (e.g. brain) can be implemented in some other machine which has a different architecture. In the latter case, what computational theory tells us is that an algorithm can be run on different machines, and in that sense, and that sense alone, the algorithm is independent of the implementation. The formal point is straightforward: since an algorithm is formal, no specific physical parameters (e.g. vacuum tubes, Ca^2+) are part of the algorithm.

That said, it is important to see that the purely formal point cannot speak to the issue of how best to discover the algorithm in fact used by a given machine, nor how best to arrive at the neurobiologically adequate task analysis. . . . The formal independence of algorithm from architecture is something we can exploit to build comp[utationally equivalent machines once we know how the brain works, but is no guide to discovery if we do not know how the brain works.

The issues of independence of levels marks a major conceptual difference between Marr (1982) and the current generation of researchers studying neural and connectionist models. In contrast to the doctrine of independence, current research suggests that considerations of implementation play a vital role in the kinds of algorithms that are devised and the kind of computational insights available. Knowledge of brain architecture, far from being irrelevant . . ., can be the essential basis and invaluable catalyst for devising powerful algorithms . . .

Levels of Organization
Marr's three-level division treats each of his levels monolithically as a single level of analysis. However, when measured against levels of organisation in the CNS, the fit is confusing and poor at best. There is organised structure at different scales: molecules, synapses, neurones, networks, layers, maps, and systems. At each structurally specified stratum, we can raise the computational question: What does that organisation of elements do?

missing p. 20-21

Levels of Processing

Lots of pages are missing, but I have a copy somewhere . . 
I will see if I can find it when I get home . . .

http://www.nsf.gov/pubs/reports/sbes_final_report.pdf
Simulation refers to the application of computational models to the study and prediction of physical events or the behavior of engineered systems. The development of computer simulation has drawn from a deep pool of scientific, mathematical, computational, and engineering knowledge and methodologies. With the depth of its intellectual development and its wide range of applications, computer simulation has emerged as a powerful tool, one that promises to revolutionize the way engineering and science are conducted in the twenty-first century.
 
Computer simulation represents an extension of theoretical science in that it is based on mathematical models. Such models attempt to characterize the  physical predictions or consequences of scientific theories. Simulation can be much more, however. For example, it can be used to explore new theories and to design new experiments to test these  theories. Simulation also provides a powerful alternative to the techniques  of experimental science and observation when phenomena are not observable or when measurements are impractical or too expensive. 

Formidable challenges stand in the way of progress in SBES research. These challenges involve resolving open problems associated with multiscale and multi-physics modeling, real-time integration of simulation methods with measurement systems, model validation and verification, handling large data, and visualization.

Hugo, You may want to look at the following manuscript, particularly Fig. 3.
http://www.mcs.anl.gov/uploads/cels/papers/P1360.pdf
Large simulations have become increasingly complex in many fields, tending to incorporate scale dependent modeling and algorithms and wide-ranging physical influences.  This  scale  of  simulation sophistication has not yet been matched in neuroscience. In this paper we describe a framework aimed at enabling natural interaction with complex simulations: their configuration, initial conditions, monitoring, and  analysis.

Brain electrical activity is generated by an immense network of billions of neurons. Because of the brain’s complexity, brain electrical activity has been studied either at the microscopic level, ranging from ion channels and single cells to small networks of cells, or at macroscopic level, in clinical measurement such  as  electroencephalogram.  Between  these two  extremes  lie  complex  interactions  between  interconnected populations of cells, interactions that are opaque to current experimental techniques. Detailed understanding of brain function and dysfunction will require multiscale simulations to span these  modeled domains: (1) employing different data models, communication patterns, and algorithms as appropriate to each layer of the simulation, (2) adaptively deploying corresponding simulation threads, and (3) matching results obtained at these different scales. Also, as the components of these simulations are developed within the computational neuroscience community, the need for a framework for testing performance scaling and verification of integrated multiscale models will become critical for several reasons: (1) understanding simulation performance scaling is prerequisite to knowing what scientific questions are practical to pursue, (2) understanding performance is critical to simulation planning activities and resource allocation, and, (3) increasingly complex simulations demand sophisticated parameter configuration management.

http://www.pnl.gov/scales/docs/volume1_300dpi.pdf
Important advances in basic science crucial to the national well-being have been brought near by a “perfect fusion” of sustained advances in scientific models, mathematical algorithms, computer architecture, and scientific software engineering. Computational simulation—a means of scientific discovery that employs a computer system to simulate a physical system according to laws derived from theory and experiment—has attained peer status with theory and experiment in many areas of science.

Computational simulation offers to enhance, as well as leapfrog, theoretical and experimental progress in many areas of science critical . . . Successes have been documented in such areas as advanced energy systems (e.g., fuel cells, fusion), biotechnology (e.g., genomics, cellular dynamics), nanotechnology (e.g., sensors, storage devices), and environmental modeling (e.g., climate prediction, pollution remediation).

The ingredients required for success in advancing scientific discovery are insights, models, and applications from scientists; theory, methods, and algorithms from mathematicians; and software and hardware infrastructure from computer scientists.
Computer scientists and mathematicians are scientists whose research in the area of large-scale computing has direction and value of its own. . . . [There is] a cascadic flow from natural scientists to mathematicians and from both to computer scientists (Figure 1). We are primarily concerned herein with the opportunities for large-scale simulation to achieve new understanding in the natural sciences. In this context, the role of the mathematical sciences is to provide a means of passing from a physical model to a discrete computational representation of that model and of efficiently manipulating that representation to obtain results whose validity is well understood. In turn, the computer sciences allow the algorithms that perform these manipulations to be executed efficiently on leading-edge computer systems.

Almost always, there are numerous ways to translate a physical model into mathematical algorithms and to implement a computational program on a given computer. Uninformed decisions made early in the process can cut off highly productive options that may arise later on. Only a bi-directional dialog, up and down the cascade, can systematically ensure that the best resources and technologies are employed to solve the most challenging scientific problems.
Its acknowledged limitations aside, the cascadic model of contemporary computational science is useful for understanding the importance of several practices in the contemporary computational research community. These include striving towards the abstraction of technologies and towards identification of universal interfaces between well-defined functional layers (enabling reuse across many applications).

Historians of science may pick different dates for the development of the twin pillars of theory and experiment in the “scientific method,” but both are ancient, and their interrelationship is well established. Each takes a turn leading the other as they mutually refine our understanding of the natural world— experiment confronting theory with new puzzles to explain and theory showing experiment where to look next to test its explanations. Unfortunately, theory and experiment both possess recognized limitations at the frontier of contemporary science.

The strains on theory were apparent to John Von Neumann (1903–1957) and drove him to develop the computational sciences of fluid dynamics, radiation transport, weather prediction, and other fields. Models of these phenomena, when expressed as mathematical equations, are inevitably large-scale and nonlinear, while the bulk of the edifice of mathematical theory (for algebraic, differential, and integral equations) is linear. Computation was to Von Neumann, and remains today, the only truly systematic means of making progress in these and many other scientific arenas. Breakthroughs in the theory of nonlinear systems come occasionally, but computational gains come steadily with increased computational power and resolution.
The strains on experimentation, the gold standard of scientific truth, have grown along with expectations for it (Figure 2). Unfortunately, many systems and many questions are nearly inaccessible to experiment. (We subsume under “experiment” both experiments designed and conducted by scientists, and observations of natural phenomena out of the direct control of scientists, such as celestial events.) The experiments that scientists need to perform to answer the most pressing questions are sometimes deemed unethical (e.g., because of their impact on living beings), hazardous (e.g., because of their impact on the life- sustaining environment we have on earth), politically untenable (e.g., prohibited by treaties to which the United States is a party), difficult (e.g., requiring measurements that are too rapid or numerous to be instrumentable or time periods too long to complete), or simply expensive.

Simulation has aspects in common with both theory and experiment. It is fundamentally theoretical, in that it starts with a theoretical model—typically a set of mathematical equations. A powerful simulation capability breathes new life into theory by creating a demand for improvements in mathematical models. Simulation is also fundamentally experimental, in that upon constructing and implementing a model, one observes the transformation of inputs (or controls) to outputs (or observables).
Simulation effectively bridges theory and experiment by allowing the execution of “theoretical experiments” on systems, including those that could never exist in the physical world, such as a fluid without viscosity. Computation also bridges theory and experiment by virtue of the computer’s serving as a universal and versatile data host. Once experimental data have been digitized, they can be compared side-by- side with simulated results in visualization systems built for the latter, reliably transmitted, and retrievably archived. Moreover, simulation and experiment can complement each other by allowing a complete picture of a system that neither can provide as well alone. Some data may not be measurable with the best experimental techniques available, and some mathematical models may be too sensitive to unknown parameters to invoke with confidence. Simulation can be used to “fill in” the missing experimental fields, using experimentally measured fields as input data. Data assimilation can also be systematically employed throughout a simulation to keep it from “drifting” from measurements, thus overcoming the effect of modeling uncertainty.

Although all matter is, in fact, composed of a finite number of particles (atoms), the number of particles in a macroscopic sample of matter, on the order of a trillion particles, places simulations at macroscopic scales from “first principles” (i.e., from the quantum theory of electronic structure) well beyond any conceivable computational capability. Similar problems arise when time scales are considered. For example, the range of time scales in protein folding is 12 orders of magnitude, since a process that takes milliseconds occurs in molecular dance steps that last femtoseconds (a trillion times shorter)—again, far too wide a range to routinely simulate using first principles.

Scientific simulation is a vertically integrated process, as indicated in Figures 1 and 3. It is also an iterative process, in which computational scientists traverse several times a hierarchy of issues in modeling, discretization, solution, code implementation, hardware execution, visualization and interpretation of results, and comparison to expectations (theories, experiments, or other simulations). They do this to develop a reliable “scientific instrument” for simulation, namely a marriage of a code with a computing and user environment. This iteration is indicated in the two loops (light arrows) of Figure 4 (reproduced from the SciDAC report).One loop over this computational process is related to validation of the model and verification that the process is correctly executing the model. With slight violence to grammar and with some oversimplification, validation poses the question “Are we solving the right equations?” and verification, the question “Are we solving the equations right?”
The other loop over the process of simulation is related to algorithm and performance tuning. A simulation that yields high-fidelity results is of little use if it is too expensive to run or if it cannot be scaled up to the resolutions, simulation times, or ensemble sizes required to describe the real-world phenomena of interest. Again with some oversimplification, algorithm tuning poses the question, “Are we getting enough science per operation?” and performance tuning, “Are we getting enough operations per cycle?”
It is clear from Figure 4 [in figures folder] that a scientific simulation depends upon numerous components, all of which have to work properly. For example, the theoretical model must not overlook or misrepresent any physically important effect. The model must not be parameterized with data of uncertain quality or provenance. The mathematical translation of the theoretical model (which may be posed as a differential equation, for instance) to a discrete representation that a computer can manipulate (a large system of algebraic relationships based on the differential equation, for instance) inevitably involves some approximation. This approximation must be controlled by a priori analysis or a posteriori checks. The algorithm that solves the discrete representation of the system being modeled must reliably converge. The compiler, operating system, and message- passing software that translate the arithmetic manipulations of the solution algorithm into loads and stores from memory to registers, or into messages that traverse communication links in a parallel computer, must be reliable and fault- tolerant. They must be employed correctly by programmers and simulation scientists so that the results are well defined and repeatable. In a simulation that relies on multiple interacting models (such as fluids and structures in adjacent domains, or radiation and combustion in the same domain), information fluxes at the interface of the respective models, which represent physical fluxes in the simulated phenomenon, must be consistent. In simulations that manipulate or produce enormous quantities of data on multiple disks, files must not be lost or mistakenly overwritten, nor misfiled results extracted for visualization. Each potential pitfall may require a different expert to detect it, control it at an acceptable level, or completely eliminate it as a possibility in future designs. These are some of the activities that exercise the left-hand loop in Figure 4.
During or after the validation of a simulation code, attention must be paid to the performance of the code in each hardware/system software environment available to the research team. Efforts to control discretization and solution error in a simulation may grossly increase the number of operations required to execute a simulation, far out of proportion to their benefit. Similarly, “conservative” approaches to moving data within a processor or between processors, to reduce the possibility of errors or miscommunication, may require extensive synchronization or minimal data replication and may be inefficient on a modern supercomputer. More elegant, often adaptive, strategies must be adopted for the simulation to be execution-worthy on an expensive massively parallel computer, even though these strategies may require highly complex implementation. These are some of the activities that exercise the right- hand loop in Figure 4.

http://www.mitre.org/work/tech_papers/tech_papers_06/06_1058/
Multiscale definition of a system

Science.241.1299.pdf (1988)
The ultimate aim of computational neuroscience is to explain how electrical and chemical signals are used in the brain to represent and process information.

. . . we do not yet understand how the nervous system enables us to see and hear, to learn skills, and remember events, to plan actions and make choices.

Explaining higher functions is difficult, in part, because nervous systems have many levels of organisation between the molecular and systems levels, each with its own important functions.

Modeling promises to be an important adjunct to . . . experimental techniques and essential in addressing the conceptual issues that arise when one studies information-processing in the brain (15-17).

[Realistic and simplifying brain models] are really end points of a continuum, and any given model may have features of both. Thus, we expect future brain models to be intermediate types that combine the advantages of both realistic and simplifying models.

It may be premature to predict how computational models will develop within neuroscience over the next decade, but several general features are already emerging. First, in view of the many different structural levels of organisation in the brain, and the realisation that models rarely span more than two levels, we expect that many different types of models will be needed. It will be especially difficult to find a chain of models to cross the gap of at least three identified levels between the cellular and systems levels of investigation. Second, a model of an intermediate level of organisation will necessarily simplify with respect to the structural properties of lower level elements, although it ought to try to incorporate as many of that level's functional properties as actually figure in the higher level's computational tasks. Thus, a model of a large network of neurones will necessarily simplify the molecular level within a single neuron.

http://arxiv.org/abs/cs/0604072
The traditional scientific method, which is based on analysis, isolation, and the gathering of complete information about a phenomenon, is incapable to deal with such complex interdependencies.

Until the early 20th century, classical mechanics, as first formulated by Newton and further developed by Laplace and others, was seen as the foundation for science as a whole. It was expected that the observations made by other sciences would sooner or later be reduced to the laws of mechanics. Although that never happened, other disciplines, such as biology, psychology or economics, did adopt a general mechanistic or Newtonian methodology and world view. This influence was so great, that most people with a basic notion of science still implicitly equate “scientific thinking” with “Newtonian thinking”. The reason for this pervasive influence is that the mechanistic
paradigm is compelling by its simplicity, coherence and apparent completeness. Moreover, it was not only very successful in its scientific applications, but largely in agreement with intuition and common-sense. Later theories of mechanics, such as relativity theory and quantum mechanics, while at least as successful in the realm of applications, lacked this simplicity and intuitive appeal, and are still plagued by paradoxes, confusions and multiple interpretations.
The logic behind Newtonian science is easy to formulate, although its implications are subtle. Its best known principle, which was formulated by the philosopher-scientist Descartes well before Newton, is that of analysis or reductionism: to understand any complex phenomenon, you need to take it apart, i.e. reduce it to its individual components. If these are still complex, you need to take your analysis one step further, and look at their components.
If you continue this subdivision long enough, you will end up with the smallest possible parts, the atoms (in the original meaning of “indivisibles”), or what we would now call “elementary particles”. Particles can be seen as separate pieces of the same hard, permanent substance that is called matter. Newtonian ontology therefore is materialistic: it assumes that all phenomena, whether physical, biological, mental or social, are ultimately constituted of matter.

The elements of the Newtonian ontology are matter, the absolute space and time in which that matter moves, and the forces or natural laws that govern movement. No other fundamental categories of being, such as mind, life, organization or purpose, are acknowledged. They are at most to be seen as epiphenomena, as particular arrangements of particles in space and time.
Newtonian epistemology is based on the reflection-correspondence view of knowledge (Turchin, 1990): our knowledge is merely an (imperfect) reflection of the particular arrangements of matter outside of us. The task of science is to make the mapping or correspondence between the external, material objects and the internal, cognitive elements (concepts or symbols) that represent them as accurate as possible. That can be achieved by simple observation, where information about external phenomena is collected and registered, thus further completing the internal picture that is taking shape. In the limit, this should lead to a perfect, objective representation of the world outside us, which would allow us to accurately predict all phenomena.

In essence, the philosophy of Newtonian science is one of simplicity: the complexity of the world is only apparent; to deal with it you need to analyse phenomena into their simplest components. Once you have done that, their evolution will turn out to be perfectly regular, reversible and predictable, while the knowledge you gained will merely be a reflection of that pre-existing order.

it is surprising that science has ignored emergence and holism for so long. One reason is that the Newtonian approach was so successful compared to its non- scientific predecessors that it seemed that its strategy of reductionism would sooner or later overcome all remaining obstacles. Another reason is that the alternative, holism or emergentism, seemed to lack any serious scientific foundation, referring more to mystical traditions than to mathematical or experimental methods.

This changed with the formulation of systems theory by Ludwig von Bertalanffy (1973). The biologist von Bertalanffy was well-versed in the mathematical models used to describe physical systems, but noted that living systems, unlike their mechanical counterparts studied by Newtonian science, are intrinsically open: they have to interact with their environment, absorbing and releasing matter and energy in order to stay alive. One reason Newtonian models were so successful in predicting was because they only considered systems, such as the planetary system, that are essentially closed. Open systems, on the other hand, depend on an environment much larger and more complex than the system itself, so that its effect can never be truly controlled or predicted.
The idea of open system immediately suggests a number of fundamental concepts that help us to give holism a more precise foundation. First, each system has an environment, from which it is separated by a boundary. This boundary gives the system its own identity, separating it from other systems. Matter, energy and information are exchanged across that boundary. Incoming streams determine the system’s input, outgoing streams its output. This provides us with a simple way to connect or couple different systems: it suffices that the output of one system be used as input by another system. A group of systems coupled via different input-output relations forms a network. If this network functions in a sufficiently coherent manner, we will consider it as a system in its own right, a supersystem, that contains the initial systems as its subsystems.From the point of view of the new system, a subsystem or component should be seen not as an independent element, but as a particular type of relation mapping input onto output. This transformation or processing can be seen as the function that this subsystem performs within the larger whole. Its internal structure or substance can be considered wholly irrelevant to the way it performs that function.

By making abstraction of the concrete substance of components, systems theory can establish isomorphisms between systems of different types, noting that the network of relations that defines them are the same at some abstract level, even though the systems at first sight belong to completely different domains.

Every system contains subsystems, while being contained in one or more supersystems. Thus, it forms part of a hierarchy which extends upwards towards ever larger wholes, and downwards towards ever smaller parts (de Rosnay, 1979).

According to cybernetics, knowledge is intrinsically subjective; it is merely an imperfect tool used by an intelligent agent to help it achieve its personal goals (Heylighen & Joslyn, 2001; Maturana & Varela, 1992). Such an agent not only does not need an objective reflection of reality, it can never achieve one. Indeed, the agent does not have access to any “external reality”: it can merely sense its inputs, note its outputs (actions) and from the correlations between them induce certain rules or regularities that seem to hold within its environment. Different agents, experiencing different inputs and outputs, will in general induce different correlations, and therefore develop a different knowledge of the environment in which they live. There is no objective way to determine whose view is right and whose is wrong, since the agents effectively live in different environments ("Umwelts")—although they may find that some of the regularities they infer appear to be similar.

For centuries, the world view underlying science has been Newtonian. The corresponding philosophy has been variously called reductionism, mechanicism or modernism. Ontologically, it reduces all phenomena to movements of independent, material particles governed by deterministic laws. Epistemologically, it holds the promise of complete, objective and certain knowledge of past and future. However, it
ignores or even denies any idea of value, ethics, or creative processes, describing the universe as merely a complicated clockwork mechanism.
Over the past century, various scientific developments have challenged this simplistic picture, gradually replacing it by one that is complex at the core. First, Heisenberg’s uncertainty principle in quantum mechanics, followed by the notion of chaos in non-linear dynamics, showed that the world is intrinsically unpredictable. Then, systems theory gave a scientific foundation to the ideas of holism and emergence. Cybernetics, in parallel with postmodern social science, showed that knowledge is intrinsically subjective. Together with the theories of self-organization and biological evolution, they moreover made us aware that regularity or organization is not given, but emerges dynamically out of a tangle of conflicting forces and random fluctuations, a process aptly summarized as “order out of chaos” (Prigogine & Stengers, 1984).





